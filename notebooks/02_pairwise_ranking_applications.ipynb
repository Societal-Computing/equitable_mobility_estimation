{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-31T08:49:08.735408300Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(os.getcwd())\n",
    "parent_directory = Path.joinpath(path.parent,'src').as_posix()\n",
    "sys.path.append(str(parent_directory))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from rasterio.features import rasterize\n",
    "from shapely.geometry import mapping\n",
    "from skimage.transform import resize\n",
    "import re\n",
    "from rasterio.mask import mask\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "from typing import List, Tuple, Dict, Any, Union\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from utils import *\n",
    "from notebook_functions import *\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "import random\n",
    "import rasterio\n",
    "\n",
    "from models import get_model\n",
    "\n",
    "import itertools\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RankerDataset, Resuable Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def get_date(image_date):\n",
    "    year = image_date[:4]\n",
    "    month = image_date[4:6]\n",
    "    day = image_date[6:]\n",
    "    ml_date = datetime.strptime(f\"{year}-{month}-{day}\", \"%Y-%m-%d\").date()\n",
    "    return ml_date\n",
    "\n",
    "def get_day_of_week(date):\n",
    "    date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    day = date.strftime(\"%A\")\n",
    "    day = day.lower()\n",
    "    return day\n",
    "\n",
    "def get_pairs_saturday(anchor_image, anchor_pair):\n",
    "    pairs_dict = {}\n",
    "    pairs_dict[\"anchor_image\"] = []\n",
    "    pairs_dict['anchor_image_pair'] = []\n",
    "    pairs_dict['label'] = []\n",
    "    postive_pairs = [(sat_image, sun_image) for sat_image in anchor_image for sun_image in anchor_pair]\n",
    "    for pair in postive_pairs:\n",
    "        pairs_dict[\"anchor_image\"].append(pair[0])\n",
    "        pairs_dict[\"anchor_image_pair\"].append(pair[1])\n",
    "        pairs_dict[\"label\"].append(1)\n",
    "    \n",
    "    negative_pairs = [(sun_image, sat_image) for sun_image in anchor_pair for sat_image in anchor_image]\n",
    "    for pair in negative_pairs:\n",
    "        pairs_dict[\"anchor_image\"].append(pair[0])\n",
    "        pairs_dict[\"anchor_image_pair\"].append(pair[1])\n",
    "        pairs_dict[\"label\"].append(0)\n",
    "        \n",
    "    pairs_df = pd.DataFrame(pairs_dict)\n",
    "    return pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanetRankerInfDataset(Dataset):\n",
    "    def __init__(self, im_data, transform=None, preprocess_type='none', building_polygon_path=None):\n",
    "        if isinstance(im_data, str):\n",
    "            image_dataframe = pd.read_csv(im_data)\n",
    "        elif isinstance(im_data, pd.DataFrame):\n",
    "            image_dataframe = im_data.reset_index(drop=True)\n",
    "\n",
    "        self.image_dataframe = image_dataframe\n",
    "        self.transform = transform\n",
    "        self.preprocess_type = preprocess_type\n",
    "        self.image_cache = {}\n",
    "        self.building_polygon_path = building_polygon_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_dataframe)\n",
    "\n",
    "    def stacking_rgb_images(self, image):\n",
    "        if image.shape[0] == 8:\n",
    "            red = image[5, :, :] \n",
    "            green = image[3, :, :]\n",
    "            blue = image[1, :, :]\n",
    "        elif image.shape[0] == 4:\n",
    "            red = image[2, :, :]\n",
    "            green = image[1, :, :]\n",
    "            blue = image[0, :, :]\n",
    "\n",
    "        return np.dstack((red, green, blue)).astype('float32')\n",
    "\n",
    "    def remove_building_polygons(self, parking_lot_img_path, building_polygon_path = \"../../input/pheonix_car_lots/building_polygons/\"):\n",
    "        parking_lot_name = Path(parking_lot_img_path).parent.stem\n",
    "        building_polygon_path = f\"{building_polygon_path}/{parking_lot_name}_buildings.geojson\"\n",
    "\n",
    "        with rasterio.open(parking_lot_img_path) as src:\n",
    "            parking_lot_img = src.read()\n",
    "            parking_lot_img = convert_to_rgb(parking_lot_img)\n",
    "            parking_lot_img_original = parking_lot_img.copy()\n",
    "            profile = src.profile\n",
    "\n",
    "        # Load the building polygon\n",
    "        buildings = gpd.read_file(building_polygon_path)\n",
    "\n",
    "        # Ensure the CRS matches the image CRS\n",
    "        buildings = buildings.to_crs(profile[\"crs\"])\n",
    "        #print(parking_lot_img.shape[:2])\n",
    "        # Rasterize the building polygon (set building areas to 1, else 0)\n",
    "        building_mask = rasterize(\n",
    "            [(geom, 1) for geom in buildings.geometry],\n",
    "            out_shape=parking_lot_img.shape[:2],\n",
    "            transform=profile[\"transform\"],\n",
    "            fill=0,\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "        # Replace building pixels with zero in the parking lot image\n",
    "        parking_lot_img[building_mask == 1] = 0\n",
    "\n",
    "        return parking_lot_img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.image_dataframe.iloc[index]\n",
    "        anchor_image_image_path = row['anchor_image']\n",
    "        anchor_image_pair_image_path = row['anchor_image_pair']\n",
    "\n",
    "        with rasterio.open(anchor_image_image_path, 'r') as src:\n",
    "            anchor_img_data = src.read()\n",
    "            \n",
    "        with rasterio.open(anchor_image_pair_image_path, 'r') as src:\n",
    "            anchor_pair_img_data = src.read()\n",
    "\n",
    "        if self.preprocess_type == 'derivatives':\n",
    "            anchor_img_data = convert_to_image_derivatives(anchor_img_data)\n",
    "            anchor_pair_img_data = convert_to_image_derivatives(anchor_pair_img_data)\n",
    "        elif self.preprocess_type == 'rgb':\n",
    "            if self.building_polygon_path:\n",
    "                print(\"Removing building polygons ....................\")\n",
    "                anchor_img_data = self.remove_building_polygons(anchor_image_image_path, self.building_polygon_path)\n",
    "                anchor_pair_img_data = self.remove_building_polygons(anchor_image_pair_image_path, self.building_polygon_path)\n",
    "            else:\n",
    "                anchor_img_data = convert_to_rgb(anchor_img_data)\n",
    "                anchor_pair_img_data = convert_to_rgb(anchor_pair_img_data)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid preprocess type, choose from ['derivatives', 'rgb']\")\n",
    "\n",
    "        if self.transform:\n",
    "            anchor_img_data_tensor = self.transform(anchor_img_data)\n",
    "            anchor_pair_img_data_tensor = self.transform(anchor_pair_img_data)\n",
    "        else:\n",
    "            anchor_img_data_tensor = torch.as_tensor(anchor_img_data)\n",
    "            anchor_pair_img_data_tensor = torch.as_tensor(anchor_pair_img_data)\n",
    "\n",
    "        return anchor_img_data_tensor, anchor_pair_img_data_tensor\n",
    "\n",
    "from typing import Dict\n",
    "def get_inference_dataloader(test_data_path: str, test_augmentations: Dict, preprocess_type: str, batch_size: int, building_polygon_path: str = None):\n",
    "    test_transform = get_transforms(test_augmentations)\n",
    "\n",
    "    if isinstance(test_data_path, str):\n",
    "        test_df = pd.read_csv(test_data_path)\n",
    "    elif isinstance(test_data_path, pd.DataFrame):\n",
    "        test_df = test_data_path.copy()\n",
    "    else:\n",
    "        raise ValueError(\"test_data_path should be either a string or a dataframe\")\n",
    "\n",
    "    test_dataset = PlanetRankerInfDataset(test_df, transform=test_transform, preprocess_type=preprocess_type, building_polygon_path=building_polygon_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "def load_model(model_config, model_weight):\n",
    "    model = get_model(model_config)\n",
    "    logger.info(f\"Loading model from {model_weight}\")\n",
    "    model.load_state_dict(torch.load(model_weight))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m     ml_date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mstrptime(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mday\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdate()\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ml_date\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclean_data_by_size\u001b[39m(img_path: \u001b[43mList\u001b[49m[\u001b[38;5;28mstr\u001b[39m]):\n\u001b[0;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Cleaning the data by removing the images which are not of the most common size'''\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial size of dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(img_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "def has_2020_images(df):\n",
    "    if 2020 in df['year'].unique():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_2020_images(df):\n",
    "    covid_images = df[(df['year'] >= 2020) & (df['year'] <= 2022)]['image_path'].values\n",
    "    if isinstance(covid_images, np.ndarray):\n",
    "        return covid_images\n",
    "    else:\n",
    "        return [covid_images]\n",
    "\n",
    "def get_pair_df(iterr_ls):\n",
    "    pairs_dict = {}\n",
    "    pairs_dict[\"anchor_image\"] = []\n",
    "    pairs_dict['anchor_image_pair'] = []\n",
    "    for pair in iterr_ls:\n",
    "        pairs_dict[\"anchor_image\"].append(pair[0])\n",
    "        pairs_dict[\"anchor_image_pair\"].append(pair[1])\n",
    "    pairs_df = pd.DataFrame(pairs_dict)\n",
    "    return pairs_df\n",
    "\n",
    "def ranker_inference_hard(pairs_df, anchor_date, multiple_lots=False):\n",
    "\n",
    "    if multiple_lots:\n",
    "        all_data_pred_ls = []\n",
    "        for parking_lot in pairs_df['parking_lot_name'].unique():\n",
    "            sel_pairs_df = pairs_df[pairs_df['parking_lot_name'] == parking_lot]\n",
    "            print(f\"Processing {parking_lot}, with size {sel_pairs_df.shape}\")\n",
    "            date_pred_scores = {}\n",
    "            date_pred_scores['date'] = []\n",
    "            date_pred_scores['probs'] = []\n",
    "            date_pred_scores['raw_rank'] = []\n",
    "            date_pred_scores['rank_scaled'] = []\n",
    "            unique_anchor_dates = sel_pairs_df[f'{anchor_date}'].unique()\n",
    "            for date in unique_anchor_dates:\n",
    "                date_df = sel_pairs_df[sel_pairs_df[f'{anchor_date}'] == date]\n",
    "                count_1 = date_df[date_df['pred'] == 1].shape[0]\n",
    "                date_pred_scores['date'].append(date)\n",
    "                date_pred_scores['probs'].append(date_df['probs'].mean())\n",
    "                date_pred_scores['raw_rank'].append(count_1)\n",
    "                date_pred_scores['rank_scaled'].append(count_1 / date_df.shape[0])\n",
    "            date_pred_scores_df = pd.DataFrame(date_pred_scores)\n",
    "            date_pred_scores_df['parking_lot_name'] = parking_lot\n",
    "            all_data_pred_ls.append(date_pred_scores_df)\n",
    "        overall_date_pred_scores_df = pd.concat(all_data_pred_ls)\n",
    "    else:\n",
    "        unique_anchor_dates = pairs_df[f'{anchor_date}'].unique()\n",
    "        date_pred_scores = {}\n",
    "        date_pred_scores['date'] = []\n",
    "        date_pred_scores['probs'] = []\n",
    "        date_pred_scores['raw_rank'] = []\n",
    "        date_pred_scores['rank_scaled'] = []        \n",
    "        for date in unique_anchor_dates:\n",
    "            date_df = pairs_df[pairs_df[f'{anchor_date}'] == date]\n",
    "            count_1 = date_df[date_df['pred'] == 1].shape[0]\n",
    "            date_pred_scores['date'].append(date)\n",
    "            date_pred_scores['probs'].append(date_df['probs'].mean())\n",
    "            date_pred_scores['raw_rank'].append(count_1)\n",
    "            date_pred_scores['rank_scaled'].append(count_1 / date_df.shape[0])\n",
    "\n",
    "        overall_date_pred_scores_df = pd.DataFrame(date_pred_scores)\n",
    "        overall_date_pred_scores_df = overall_date_pred_scores_df.sort_values('date')\n",
    "    \n",
    "    overall_date_pred_scores_df['rank'] = overall_date_pred_scores_df['raw_rank'].rank(ascending=True, method=\"dense\").astype(int)\n",
    "    return overall_date_pred_scores_df\n",
    "\n",
    "\n",
    "def remove_img_with_no_clear_udm(udm_img, masked_area):\n",
    "    '''\n",
    "    checks if the image has no clear pixels in it then returns True else False\n",
    "    True, means te image should be removed else it should be kept by returning False\n",
    "    '''\n",
    "    unique_values, value_counts = np.unique(udm_img[0], return_counts=True)\n",
    "    clear_channel_value_counts_dict = dict(zip(unique_values, value_counts))\n",
    "\n",
    "    masked_unique_values, masked_value_counts = np.unique(masked_area, return_counts=True)\n",
    "    masked_value_counts_dict = dict(zip(masked_unique_values, masked_value_counts))\n",
    "\n",
    "    if 1 in clear_channel_value_counts_dict.keys() and 1 in masked_value_counts_dict.keys():\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def get_date(image_date):\n",
    "    year = image_date[:4]\n",
    "    month = image_date[4:6]\n",
    "    day = image_date[6:]\n",
    "    ml_date = datetime.strptime(f\"{year}-{month}-{day}\", \"%Y-%m-%d\").date()\n",
    "    return ml_date\n",
    "\n",
    "def clean_data_by_size(img_path: List[str]):\n",
    "    '''Cleaning the data by removing the images which are not of the most common size'''\n",
    "    logger.info(f\"Initial size of dataset: {len(img_path)}\")\n",
    "    size = check_size_most(img_path)\n",
    "    logger.info(f\"Most common size of the dataset: {size}\")\n",
    "    for i in img_path:\n",
    "        img = rioxarray.open_rasterio(i)\n",
    "        if img.shape != size:\n",
    "            img_path.remove(i)\n",
    "    logger.info(f\"Final size of dataset(after removing faulty size images): {len(img_path)}\")\n",
    "    return img_path\n",
    "\n",
    "\n",
    "def clean_data_by_black_pixels(img_path_list):\n",
    "    black_pixels = []\n",
    "    for i in img_path_list:\n",
    "        img = rioxarray.open_rasterio(i)\n",
    "        black_pixels.append(np.count_nonzero(img.data[0:3].transpose(1, 2, 0) == 0))\n",
    "    img_path_median = []\n",
    "    for i in range(len(img_path_list)):\n",
    "        if black_pixels[i] <= np.median(black_pixels):\n",
    "            img_path_median.append(img_path_list[i])\n",
    "    logger.info(f\"Final size of dataset(after removing faulty black pixels in images): {len(img_path_median)}\")\n",
    "    return img_path_median\n",
    "\n",
    "\n",
    "def clean_data_by_white_pixels(img_path_list, threshold=200):\n",
    "    white_pixels = []\n",
    "    for i in img_path_list:\n",
    "        img = rioxarray.open_rasterio(i)\n",
    "        img = convert_to_rgb(img.data)\n",
    "        white_pixels.append(np.count_nonzero(img >= threshold))\n",
    "\n",
    "    img_path_mean_white = []\n",
    "    for i in range(len(img_path_list)):\n",
    "        if white_pixels[i] <= np.mean(white_pixels):\n",
    "            img_path_mean_white.append(img_path_list[i])\n",
    "    logger.info(f\"Final size of dataset(after removing faulty white pixels in images) {len(img_path_mean_white)}\")\n",
    "    return img_path_mean_white\n",
    "\n",
    "\n",
    "def get_reference_image_single_parkinglot(images_path: str, shapefile_path: str):\n",
    "    '''\n",
    "    This function calculates the reference image for a single parking lot by taking the mean of all the gray images.\n",
    "    '''\n",
    "    all_images = []\n",
    "    parking_lot_gdf = gpd.read_file(shapefile_path)\n",
    "    for image_path in images_path:\n",
    "        with rasterio.open(image_path) as src:\n",
    "            image_8b = src.read()\n",
    "            parking_lot_gdf = parking_lot_gdf.to_crs(src.crs)\n",
    "            masked_data, _ = mask(src, parking_lot_gdf.geometry, crop=True)\n",
    "\n",
    "        rgb_image = convert_to_rgb(image_8b)\n",
    "        gray_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2GRAY)\n",
    "        masked_data = masked_data[0]\n",
    "        masked_data = np.where(masked_data == 0, 0, 1)\n",
    "        gray_image = gray_image * masked_data\n",
    "\n",
    "        gray_image_resized = resize(gray_image, (32, 32))\n",
    "        all_images.append(gray_image_resized)\n",
    "\n",
    "    all_images_np = np.array(all_images)\n",
    "    reference_image = all_images_np.mean(axis=0)\n",
    "    return reference_image\n",
    "\n",
    "\n",
    "def calculate_histogram(image, bins=256):\n",
    "    '''\n",
    "    Calculate the histogram of the image\n",
    "    '''\n",
    "    hist = cv2.calcHist([image], [0], None, [bins], [0, 256])\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "    return hist\n",
    "\n",
    "def compare_histograms(hist1: np.ndarray, hist2: np.ndarray):\n",
    "    '''\n",
    "    Compare the histograms of two images\n",
    "    '''\n",
    "    return np.sum(np.abs(hist1 - hist2))\n",
    "\n",
    "def cleaning_by_histogram_single_parkinglot(filtered_df_by_udm , shapefile_path: str, cluster_name: str, parking_lot_name: str):\n",
    "    threshold = 0.45\n",
    "    filter_images_by_size = clean_data_by_size(filtered_df_by_udm['image_path'].tolist())\n",
    "    reference_image = get_reference_image_single_parkinglot(filter_images_by_size, shapefile_path)\n",
    "    images_path = filter_images_by_size\n",
    "    if reference_image.ndim == 2:\n",
    "        reference_image = reference_image[np.newaxis, ...]\n",
    "\n",
    "    reference_image = reference_image.astype(np.uint8)\n",
    "    ref_hist = calculate_histogram(reference_image)\n",
    "    filtered_images = []\n",
    "    parking_lot_gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    for image_path in images_path:\n",
    "        with rasterio.open(image_path) as src:\n",
    "            image_8b = src.read()\n",
    "            parking_lot_gdf = parking_lot_gdf.to_crs(src.crs)\n",
    "            masked_data, _ = mask(src, parking_lot_gdf.geometry, crop=True)\n",
    "\n",
    "        image = convert_to_rgb(image_8b)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        masked_data = masked_data[0]\n",
    "        masked_data = np.where(masked_data == 0, 0, 1)\n",
    "        image = image * masked_data\n",
    "\n",
    "        image = image.astype(np.uint8)\n",
    "        matched_image = image[np.newaxis, ...]\n",
    "        hist = calculate_histogram(matched_image)\n",
    "        similarity = compare_histograms(ref_hist, hist)\n",
    "        #print(f\"Similarity score : {similarity}\")\n",
    "        if similarity <= threshold:\n",
    "            filtered_images.append(\n",
    "                {'cluster': cluster_name, 'parking_lot_name': parking_lot_name, 'image_path': image_path})\n",
    "\n",
    "    filtered_images_df = pd.DataFrame(filtered_images)\n",
    "    return filtered_images_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sudan Use Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def get_date(image_date):\n",
    "    year = image_date[:4]\n",
    "    month = image_date[4:6]\n",
    "    day = image_date[6:]\n",
    "    ml_date = datetime.strptime(f\"{year}-{month}-{day}\", \"%Y-%m-%d\").date()\n",
    "    return ml_date\n",
    "\n",
    "all_sudan =  glob.glob(\"../../input/sudan/polygon_images/sudan_clipped/**/*.tif\", recursive=True)\n",
    "all_sudan = [image for image in all_sudan if 'udm2' not in image]\n",
    "all_sudan_dict = {}\n",
    "all_sudan_dict['image_path'] = []\n",
    "all_sudan_dict['date'] = []\n",
    "\n",
    "for image in all_sudan:\n",
    "    date = Path(image).stem.split(\"_\")[0]\n",
    "    all_sudan_dict['image_path'].extend([image])\n",
    "    all_sudan_dict['date'].extend([get_date(date)])\n",
    "\n",
    "all_sudan_df = pd.DataFrame(all_sudan_dict)\n",
    "all_sudan_df['cluster'] = 'sudan'\n",
    "all_sudan_df.to_csv(\"../../input/sudan/polygon_images/sudan_clipped/all_images.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_sudan_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View all images from Sudan Jackdon Parking lot beofore ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sudan_df= pd.read_csv(\"../../input/sudan/polygon_images/sudan_clipped/all_images.csv\")\n",
    "rows, cols = 10, 16  # 10x16 grid\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 12))  # Adjust figure size\n",
    "\n",
    "# Iterate over images and plot them\n",
    "for idx, (ax, (_, row)) in enumerate(zip(axes.flatten(), all_sudan_df.iterrows())):\n",
    "    image_path = row['image_path']\n",
    "    \n",
    "    # Read and process image\n",
    "    with rasterio.open(image_path, 'r') as src:\n",
    "        img_data = src.read()\n",
    "        img_data = convert_to_rgb(img_data)  # Convert from (bands, H, W) to (H, W, bands)\n",
    "\n",
    "    # Display image\n",
    "    ax.imshow(img_data)\n",
    "    \n",
    "    # Extract date from filename\n",
    "    date = row['date']  # Assuming filename contains date\n",
    "    \n",
    "    ax.set_title(date, fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign pre and post war labels based on days observed by \n",
    "Guo, Zhe, et al. \"Monitoring indicators of economic activities in Sudan amidst ongoing conflict using satellite data.\" Defence and Peace Economics 35.8 (2024): 992-1008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sudan_df = pd.read_csv(\"../../input/sudan/polygon_images/sudan_clipped/all_images.csv\")\n",
    "all_sudan_df['date'] = pd.to_datetime(all_sudan_df['date']).dt.date\n",
    "pre_war_all_sudan_df = all_sudan_df[(all_sudan_df['date'] >= datetime.strptime('2023-04-01', \"%Y-%m-%d\").date()) & (all_sudan_df['date'] <= datetime.strptime('2023-04-07', \"%Y-%m-%d\").date())]\n",
    "post_war_all_sudan_df = all_sudan_df[(all_sudan_df['date'] >= datetime.strptime('2023-04-14', \"%Y-%m-%d\").date()) & (all_sudan_df['date'] <= datetime.strptime('2023-04-21', \"%Y-%m-%d\").date())]\n",
    "#current_all_sudan_df = all_sudan_df[(all_sudan_df['date'] >= datetime.strptime('2025-01-01', \"%Y-%m-%d\").date())]\n",
    "pre_war_all_sudan_df['period'] = 'pre_war'\n",
    "post_war_all_sudan_df['period'] = 'post_war'\n",
    "#current_all_sudan_df['period'] = 'current'\n",
    "all_sudan_df = pd.concat([pre_war_all_sudan_df, post_war_all_sudan_df])#, current_all_sudan_df])\n",
    "all_sudan_df = all_sudan_df.reset_index(drop=True)\n",
    "all_sudan_df['date'] = pd.to_datetime(all_sudan_df['date']).dt.date.astype(str)\n",
    "all_sudan_df['day_of_week'] = all_sudan_df['date'].apply(get_day_of_week)\n",
    "print(all_sudan_df.shape)\n",
    "\n",
    "all_sudan_df[\"date\"] = pd.to_datetime(all_sudan_df[\"date\"])\n",
    "all_sudan_df[\"day_of_week\"] = all_sudan_df[\"date\"].dt.day_name()\n",
    "day_ranked_df_counts = all_sudan_df.groupby([\"period\", \"day_of_week\"])[\"date\"].count().unstack(fill_value=0)\n",
    "print(day_ranked_df_counts)\n",
    "all_sudan_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only days that are presnt in both pre and post war data\n",
    "all_sudan_df = all_sudan_df[all_sudan_df['day_of_week'].isin(['Saturday', 'Sunday', 'Monday','Tuesday', 'Wednesday'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"../../experiments/20250325-013240_patch_pairwiserankerwithparameter/inference_config.yaml\")\n",
    "model = load_model(config['model_config'], f\"../../{config['model_config']['model_weight']}\")\n",
    "\n",
    "all_sudan_df = all_sudan_df.groupby(['date','cluster']).agg({'image_path':'first'}).reset_index()\n",
    "print(all_sudan_df.shape)\n",
    "all_sudan_df['date'] = pd.to_datetime(all_sudan_df['date']).dt.date.astype(str)\n",
    "all_sudan_df['day_of_week'] = all_sudan_df['date'].apply(lambda x: get_day_of_week(x))\n",
    "# day_of_week = 'wednesday'\n",
    "# all_sudan_df_saturday = all_sudan_df[all_sudan_df['day_of_week'] == day_of_week]\n",
    "\n",
    "pairs = list(itertools.product(all_sudan_df['image_path'], repeat=2))\n",
    "\n",
    "covid_anchor_images = [pair for pair in pairs if pair[0] != pair[1]]\n",
    "\n",
    "pairs_sudan_df = pd.DataFrame(covid_anchor_images, columns=['anchor_image', 'anchor_image_pair'])\n",
    "\n",
    "pairs_sudan_df['anchor_date'] = pairs_sudan_df.anchor_image.apply(lambda x: get_date(Path(x).stem.split(\"_\")[0]))\n",
    "pairs_sudan_df['anchor_day'] = pairs_sudan_df['anchor_date'].apply(lambda x: x.strftime(\"%A\")).astype(str)\n",
    "pairs_sudan_df['anchor_date'] = pairs_sudan_df['anchor_date'].astype(str)\n",
    "pairs_sudan_df['anchor_pair_date'] = pairs_sudan_df.anchor_image_pair.apply(lambda x: get_date(Path(x).stem.split(\"_\")[0]))\n",
    "pairs_sudan_df['anchor_pair_date'] = pairs_sudan_df['anchor_pair_date'].astype(str)\n",
    "\n",
    "test_loader = get_inference_dataloader(pairs_sudan_df, config['data_config']['test_data']['augmentations'], 'rgb', 1)\n",
    "pair_wise_test_dict = {}\n",
    "pair_wise_test_dict['probs'] = []\n",
    "\n",
    "for anchor_image, anchor_image_pair in test_loader:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(anchor_image, anchor_image_pair)\n",
    "        pair_wise_test_dict['probs'].extend(output.cpu().numpy()[0])\n",
    "\n",
    "pairs_sudan_df['probs'] = pair_wise_test_dict['probs']\n",
    "pairs_sudan_df['pred'] = np.where(pairs_sudan_df['probs'] <= 0.5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sudan_df = ranker_inference_hard(pairs_sudan_df,'anchor_date')\n",
    "ranked_sudan_df.reset_index(drop=True, inplace=True)\n",
    "ranked_sudan_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sudan_df_agg = ranked_sudan_df.copy()\n",
    "ranked_sudan_df_agg = ranked_sudan_df_agg.groupby(['date']).agg({'raw_rank': 'mean'}).reset_index()\n",
    "ranked_sudan_df_agg['rank'] = ranked_sudan_df_agg['raw_rank'].rank(ascending=True).astype(int)\n",
    "ranked_sudan_df_agg.sort_values('date', inplace=True)\n",
    "ranked_sudan_df_agg['day_of_week'] = ranked_sudan_df_agg['date'].apply(lambda x: get_day_of_week(x))\n",
    "ranked_sudan_df_agg['date'] = pd.to_datetime(ranked_sudan_df_agg['date']).dt.date\n",
    "ranked_sudan_df_agg['period'] = ranked_sudan_df_agg['date'].apply(lambda x: 'pre_war' if x <= datetime.strptime('2023-04-14', \"%Y-%m-%d\").date() else 'post_war')\n",
    "ranked_sudan_df_agg['ID'] = ranked_sudan_df_agg['period'] + '_' + ranked_sudan_df_agg['day_of_week'] + \"_\" + ranked_sudan_df_agg['date'].astype(str)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.bar(ranked_sudan_df_agg['ID'], ranked_sudan_df_agg['rank']-1)\n",
    "plt.xticks(rotation=75)\n",
    "plt.grid(True)\n",
    "plt.title(f'Image ranking based of all parking lots on dates: Sudan Bus Terminal')\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Number of images beaten')\n",
    "plt.xlabel('Date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sudan_df_agg = ranked_sudan_df.copy()\n",
    "ranked_sudan_df_agg = ranked_sudan_df_agg.groupby(['date']).agg({'raw_rank': 'mean'}).reset_index()\n",
    "ranked_sudan_df_agg['rank'] = ranked_sudan_df_agg['raw_rank'].rank(ascending=True).astype(int)\n",
    "ranked_sudan_df_agg.sort_values('date', inplace=True)\n",
    "ranked_sudan_df_agg['day_of_week'] = ranked_sudan_df_agg['date'].apply(lambda x: get_day_of_week(x))\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Saturday', 'Sunday']\n",
    "ranked_sudan_df_agg['day_of_week'] = pd.Categorical(\n",
    "    ranked_sudan_df_agg['day_of_week'].str.capitalize(),\n",
    "    categories=day_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "ranked_sudan_df_agg['date'] = pd.to_datetime(ranked_sudan_df_agg['date']).dt.date\n",
    "ranked_sudan_df_agg['period'] = ranked_sudan_df_agg['date'].apply(lambda x: 'pre_war' if x <= datetime.strptime('2023-04-14', \"%Y-%m-%d\").date() else 'post_war')\n",
    "#ranked_sudan_df_agg['ID'] = ranked_sudan_df_agg['period'] + '_' + ranked_sudan_df_agg['day_of_week'] + \"_\" + ranked_sudan_df_agg['date'].astype(str)\n",
    "#ranked_sudan_df_agg['day_of_week'] = ranked_sudan_df_agg['day_of_week'].str.capitalize()\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(data=ranked_sudan_df_agg, x='day_of_week', y='rank', hue='period', order=day_order)\n",
    "plt.legend(title='Period', loc='upper left', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.xticks(rotation=75)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Image ranking', fontsize=20)\n",
    "plt.xlabel('Day of week', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 2, 5  # 10x16 grid\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 12))  # Adjust figure size\n",
    "\n",
    "# Iterate over images and plot them\n",
    "for idx, (ax, (_, row)) in enumerate(zip(axes.flatten(), all_sudan_df.iterrows())):\n",
    "    image_path = row['image_path']\n",
    "    \n",
    "    # Read and process image\n",
    "    with rasterio.open(image_path, 'r') as src:\n",
    "        img_data = src.read()\n",
    "        img_data = convert_to_rgb(img_data)  # Convert from (bands, H, W) to (H, W, bands)\n",
    "\n",
    "    # Display image\n",
    "    ax.imshow(img_data)\n",
    "    \n",
    "    # Extract date from filename\n",
    "    date = row['date']  # Assuming filename contains date\n",
    "    \n",
    "    ax.set_title(date, fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
